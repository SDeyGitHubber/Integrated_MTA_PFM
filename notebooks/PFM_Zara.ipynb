{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bMTlkbIjDW_6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import time\n",
        "\n",
        "# Dataset Class\n",
        "class PFM_TrajectoryDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file_path, history_len=8, prediction_len=12):\n",
        "        self.data = self.load_data(file_path)\n",
        "        self.history_len = history_len\n",
        "        self.prediction_len = prediction_len\n",
        "        # Create a list of valid frame indices that have enough history and future data\n",
        "        self.valid_frames = self._get_valid_frames()\n",
        "\n",
        "    def load_data(self, file_path):\n",
        "        data = {}\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                parts = line.strip().split(',')\n",
        "                if len(parts) == 4:  # Ensure valid line\n",
        "                    frame, agent, x, y = map(float, parts)\n",
        "                    frame, agent = int(frame/10), int(agent)\n",
        "                    if frame not in data:\n",
        "                        data[frame] = {}\n",
        "                    data[frame][agent] = torch.tensor([x, y], dtype=torch.float32)\n",
        "        return data\n",
        "\n",
        "    def _get_valid_frames(self):\n",
        "        \"\"\"Get frames that have sufficient history and future data\"\"\"\n",
        "        all_frames = sorted(self.data.keys())\n",
        "        valid_frames = []\n",
        "\n",
        "        for frame in all_frames:\n",
        "            # Check if we have enough history and future frames\n",
        "            history_start = frame - self.history_len + 1\n",
        "            future_end = frame + self.prediction_len\n",
        "\n",
        "            # Ensure we have data for the required time range\n",
        "            if history_start >= min(all_frames) and future_end <= max(all_frames):\n",
        "                valid_frames.append(frame)\n",
        "\n",
        "        return valid_frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_frames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frame = self.valid_frames[idx]\n",
        "\n",
        "        # Get all agents present at this frame\n",
        "        if frame not in self.data:\n",
        "            # Return empty tensors if no data\n",
        "            return (torch.zeros(0, self.history_len, 2),\n",
        "                   torch.zeros(0, self.prediction_len, 2),\n",
        "                   torch.zeros(0, 0, 2),\n",
        "                   torch.zeros(0, 2))\n",
        "\n",
        "        agents = list(self.data[frame].keys())\n",
        "        num_agents = len(agents)\n",
        "\n",
        "        if num_agents == 0:\n",
        "            return (torch.zeros(0, self.history_len, 2),\n",
        "                   torch.zeros(0, self.prediction_len, 2),\n",
        "                   torch.zeros(0, 0, 2),\n",
        "                   torch.zeros(0, 2))\n",
        "\n",
        "        history = torch.zeros(num_agents, self.history_len, 2)\n",
        "        future = torch.zeros(num_agents, self.prediction_len, 2)\n",
        "        goals = torch.zeros(num_agents, 2)\n",
        "\n",
        "        for i, agent in enumerate(agents):\n",
        "            # Fill history (going backwards from current frame)\n",
        "            for t in range(self.history_len):\n",
        "                hist_frame = frame - (self.history_len - 1 - t)  # Fixed indexing\n",
        "                if hist_frame in self.data and agent in self.data[hist_frame]:\n",
        "                    history[i, t] = self.data[hist_frame][agent]\n",
        "                # If no data available, position remains zero (padding)\n",
        "\n",
        "            # Fill future (going forwards from next frame)\n",
        "            for t in range(self.prediction_len):\n",
        "                fut_frame = frame + t + 1  # Start from next frame\n",
        "                if fut_frame in self.data and agent in self.data[fut_frame]:\n",
        "                    future[i, t] = self.data[fut_frame][agent]\n",
        "                # If no data available, position remains zero (padding)\n",
        "\n",
        "            # Extract goal from the last timestep of future trajectory\n",
        "            # Find the last non-zero position in future, or use the last timestep\n",
        "            non_zero_mask = torch.any(future[i] != 0, dim=1)\n",
        "            if non_zero_mask.any():\n",
        "                last_valid_idx = torch.where(non_zero_mask)[0][-1]\n",
        "                goals[i] = future[i, last_valid_idx]\n",
        "            else:\n",
        "                # If no future data, use current position as goal\n",
        "                goals[i] = self.data[frame][agent]\n",
        "\n",
        "        # Collect neighbors for each agent at the current frame\n",
        "        neighbors_list = []\n",
        "        for i, agent in enumerate(agents):\n",
        "            # Get positions of all other agents at the current frame\n",
        "            agent_neighbors = []\n",
        "            for other_agent in self.data[frame]:\n",
        "                if other_agent != agent:\n",
        "                    agent_neighbors.append(self.data[frame][other_agent])\n",
        "\n",
        "            if agent_neighbors:\n",
        "                neighbors_tensor = torch.stack(agent_neighbors)\n",
        "            else:\n",
        "                # If no neighbors, create a dummy neighbor at origin\n",
        "                neighbors_tensor = torch.zeros(1, 2)\n",
        "\n",
        "            neighbors_list.append(neighbors_tensor)\n",
        "\n",
        "        # Pad neighbors to have the same number for all agents\n",
        "        if neighbors_list:\n",
        "            max_neighbors = max(n.shape[0] for n in neighbors_list)\n",
        "            padded_neighbors = torch.zeros(num_agents, max_neighbors, 2)\n",
        "\n",
        "            for i, neighbor_tensor in enumerate(neighbors_list):\n",
        "                padded_neighbors[i, :neighbor_tensor.shape[0]] = neighbor_tensor\n",
        "\n",
        "            neighbors = padded_neighbors\n",
        "        else:\n",
        "            neighbors = torch.zeros(num_agents, 1, 2)\n",
        "\n",
        "        return history, future, neighbors, goals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdqvpzMXDimg",
        "outputId": "05637af9-03e1-40b4-e53e-3eb27b70ad17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "History shape: torch.Size([3, 4, 8, 2])\n",
            "Future shape: torch.Size([3, 4, 12, 2])\n",
            "Neighbors shape: torch.Size([3, 4, 6, 2])\n",
            "Goals shape: torch.Size([3, 4, 2])\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for batches that include goal\"\"\"\n",
        "    max_agents = max(sample[0].shape[0] for sample in batch)\n",
        "    # history\n",
        "    max_neighbors = max(sample[2].shape[1] for sample in batch)  # neighbors\n",
        "    hist_len = batch[0][0].shape[1]\n",
        "    fut_len = batch[0][1].shape[1]\n",
        "\n",
        "    padded_histories = []\n",
        "    padded_futures = []\n",
        "    padded_neighbors = []\n",
        "    padded_goals = []\n",
        "\n",
        "    for sample in batch:\n",
        "        history, future, neighbors, goal = sample\n",
        "        A, H, D = history.shape\n",
        "        N = neighbors.shape[1]\n",
        "\n",
        "        padded_hist = torch.zeros(max_agents, hist_len, D)\n",
        "        padded_fut = torch.zeros(max_agents, fut_len, D)\n",
        "        padded_neigh = torch.zeros(max_agents, max_neighbors, D)\n",
        "        padded_goal = torch.zeros(max_agents, D)\n",
        "\n",
        "        padded_hist[:A] = history\n",
        "        padded_fut[:A] = future\n",
        "        padded_neigh[:A, :N] = neighbors\n",
        "        padded_goal[:A] = goal\n",
        "\n",
        "        padded_histories.append(padded_hist)\n",
        "        padded_futures.append(padded_fut)\n",
        "        padded_neighbors.append(padded_neigh)\n",
        "        padded_goals.append(padded_goal)\n",
        "\n",
        "    return (\n",
        "        torch.stack(padded_histories),   # [B, A, hist_len, D]\n",
        "        torch.stack(padded_futures),     # [B, A, fut_len, D]\n",
        "        torch.stack(padded_neighbors),   # [B, A, max_neighbors, D]\n",
        "        torch.stack(padded_goals)        # [B, A, D]\n",
        "    )\n",
        "\n",
        "# FIXED: Added goals to test data (4th element in each tuple)\n",
        "test_batch = [\n",
        "    (torch.rand(3, 8, 2), torch.rand(3, 12, 2), torch.rand(3, 5, 2), torch.rand(3, 2)),  # 3 agents with goals\n",
        "    (torch.rand(2, 8, 2), torch.rand(2, 12, 2), torch.rand(2, 3, 2), torch.rand(2, 2)),  # 2 agents with goals\n",
        "    (torch.rand(4, 8, 2), torch.rand(4, 12, 2), torch.rand(4, 6, 2), torch.rand(4, 2))   # 4 agents with goals\n",
        "]\n",
        "\n",
        "# Now this should work correctly\n",
        "hist, fut, neigh, goals = collate_fn(test_batch)  # Note: also need to unpack goals here\n",
        "print(f\"History shape: {hist.shape}\")     # Will be [3, 4, 8, 2]\n",
        "print(f\"Future shape: {fut.shape}\")       # Will be [3, 4, 12, 2]\n",
        "print(f\"Neighbors shape: {neigh.shape}\")  # Will be [3, 4, 6, 2]\n",
        "print(f\"Goals shape: {goals.shape}\")      # Will be [3, 4, 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "Gxiq8eloDioY",
        "outputId": "7a67ad87-7787-46a9-a9c2-f99847b20ee3"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "/content/crowds_zara02_test.txt not found.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1149862212.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/crowds_zara02_test.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0maverage_speed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_average_speed_from_file_ZARA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average speed: {average_speed:.4f} units per frame\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1149862212.py\u001b[0m in \u001b[0;36mcompute_average_speed_from_file_ZARA\u001b[0;34m(file_path, delimiter)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Load dataset: shape (N,4) --> frame, agent, x, y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Sort by agent then by frame to ensure proper ordering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1381\u001b[0;31m     arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n\u001b[0m\u001b[1;32m   1382\u001b[0m                 \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiplines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                 \u001b[0munpack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                 \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    530\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: /content/crowds_zara02_test.txt not found."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_average_speed_from_file_ZARA(file_path, delimiter=None):\n",
        "    \"\"\"\n",
        "    Compute average speed across all agents from trajectory data in file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the file containing data: frame agent x y\n",
        "        delimiter (str or None): delimiter for np.loadtxt (default None detects spaces).\n",
        "                                Use \",\" if your file is comma separated.\n",
        "\n",
        "    Returns:\n",
        "        float: average speed (units per frame step)\n",
        "    \"\"\"\n",
        "    # Load dataset: shape (N,4) --> frame, agent, x, y\n",
        "    data = np.loadtxt(file_path, delimiter=delimiter)\n",
        "\n",
        "    # Sort by agent then by frame to ensure proper ordering\n",
        "    data = data[np.lexsort((data[:,0], data[:,1]))]\n",
        "\n",
        "    total_distance = 0.0\n",
        "    total_transitions = 0\n",
        "\n",
        "    # Process each agent separately\n",
        "    for agent_id in np.unique(data[:,1]):\n",
        "        agent_data = data[data[:,1] == agent_id]\n",
        "\n",
        "        # Filter out zero positions (padded)\n",
        "        mask = ~((agent_data[:,2] == 0) & (agent_data[:,3] == 0))\n",
        "        agent_data = agent_data[mask]\n",
        "\n",
        "        if len(agent_data) < 2:\n",
        "            continue\n",
        "\n",
        "        frames = agent_data[:,0]\n",
        "        positions = agent_data[:, 2:4]\n",
        "\n",
        "        # Calculate Euclidean distances between consecutive positions\n",
        "        displacements = positions[1:] - positions[:-1]\n",
        "        distances = np.linalg.norm(displacements, axis=1)\n",
        "\n",
        "        # Calculate time differences (frame differences)\n",
        "        delta_times = frames[1:] - frames[:-1]\n",
        "\n",
        "        # Ignore zero or negative time intervals (avoid division issues)\n",
        "        valid_mask = delta_times > 0\n",
        "\n",
        "        total_distance += distances[valid_mask].sum()\n",
        "        total_transitions += delta_times[valid_mask].sum()  # total time elapsed\n",
        "\n",
        "    if total_transitions > 0:\n",
        "        avg_speed = total_distance / total_transitions\n",
        "    else:\n",
        "        avg_speed = 0.0\n",
        "\n",
        "    return avg_speed\n",
        "\n",
        "file_path = \"/content/crowds_zara02_test.txt\"\n",
        "average_speed = compute_average_speed_from_file_ZARA(file_path)\n",
        "print(f\"Average speed: {average_speed:.4f} units per frame\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9Tn73aH3EOMD"
      },
      "outputs": [],
      "source": [
        "def calculate_speed(trajectories):\n",
        "    \"\"\"\n",
        "    Calculate average speed from trajectory data\n",
        "    Args:\n",
        "        trajectories: [B, A, T, 2] tensor where T is time steps\n",
        "    Returns:\n",
        "        average_speed: scalar tensor\n",
        "    \"\"\"\n",
        "    B, A, T, D = trajectories.shape\n",
        "\n",
        "    # Filter out padding (0,0) points\n",
        "    non_zero_mask = ~((trajectories[:, :, :, 0] == 0) & (trajectories[:, :, :, 1] == 0))\n",
        "\n",
        "    # Calculate displacement between consecutive time steps\n",
        "    # trajectories[:, :, 1:] - trajectories[:, :, :-1] gives [B, A, T-1, 2]\n",
        "    displacements = trajectories[:, :, 1:] - trajectories[:, :, :-1]\n",
        "\n",
        "    # Calculate distances (speeds) for each time step\n",
        "    distances = torch.norm(displacements, dim=-1)  # [B, A, T-1]\n",
        "\n",
        "    # Apply mask to ignore padded transitions (both current and next point should be non-zero)\n",
        "    valid_transitions = non_zero_mask[:, :, :-1] & non_zero_mask[:, :, 1:]\n",
        "\n",
        "    # Only consider valid (non-padded) transitions\n",
        "    valid_distances = distances * valid_transitions.float()\n",
        "\n",
        "    # Calculate average speed (sum of valid distances / number of valid transitions)\n",
        "    total_distance = valid_distances.sum()\n",
        "    total_transitions = valid_transitions.sum()\n",
        "\n",
        "    if total_transitions > 0:\n",
        "        avg_speed = total_distance / total_transitions\n",
        "    else:\n",
        "        avg_speed = torch.tensor(0.0, device=trajectories.device)\n",
        "\n",
        "    return avg_speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eP5U5LXpERaw"
      },
      "outputs": [],
      "source": [
        "# === HELPER FUNCTION TO CHECK VIOLATIONS ===\n",
        "def check_speed_violations(predictions, history, min_speed, max_speed):\n",
        "    \"\"\"\n",
        "    Count the number of speed constraint violations in predictions\n",
        "    \"\"\"\n",
        "    B, A, T, D = predictions.shape\n",
        "    last_pos = history[:, :, -1, :]  # [B, A, 2]\n",
        "    current_pos = last_pos.clone()\n",
        "\n",
        "    violations = 0\n",
        "\n",
        "    for t in range(T):\n",
        "        displacement = predictions[:, :, t] - current_pos\n",
        "        speeds = torch.norm(displacement, dim=-1)\n",
        "\n",
        "        # Count violations (excluding zero speeds from padding)\n",
        "        non_zero_mask = speeds > 0\n",
        "        too_fast = (speeds > max_speed) & non_zero_mask\n",
        "        too_slow = (speeds < min_speed) & non_zero_mask\n",
        "\n",
        "        violations += (too_fast | too_slow).sum().item()\n",
        "        current_pos = predictions[:, :, t].clone()\n",
        "\n",
        "    return violations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7N7sgZPXDiqs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# === Trainable Potential Field Module ===\n",
        "class PotentialField(nn.Module):\n",
        "    def __init__(self, goal, num_agents=1000, k_init=1.0, repulsion_radius=0.5):\n",
        "        super().__init__()\n",
        "        self.register_buffer('goal', torch.tensor(goal, dtype=torch.float32))\n",
        "        self.repulsion_radius = repulsion_radius\n",
        "        self.coeff_embedding = nn.Embedding(num_agents, 3)\n",
        "        self.coeff_embedding.weight.data.fill_(k_init)\n",
        "\n",
        "    def forward(self, pos, predicted, neighbors, goal, coeffs):\n",
        "        k1, k2, kr = coeffs[..., 0:1], coeffs[..., 1:2], coeffs[..., 2:3]\n",
        "\n",
        "        # Attractive force towards goal\n",
        "        Fg = k1 * (goal - pos)\n",
        "\n",
        "        # Forward motion force (towards predicted step)\n",
        "        Fp = k2 * (predicted[:, :, 0, :] - pos)\n",
        "\n",
        "        # Repulsive force from neighbors\n",
        "        diffs = pos.unsqueeze(2) - neighbors\n",
        "        dists = torch.norm(diffs, dim=-1, keepdim=True) + 1e-6\n",
        "        mask = (dists < self.repulsion_radius).float()\n",
        "        Fr = (kr.unsqueeze(2) * diffs / dists.pow(2) * mask).sum(dim=2)\n",
        "\n",
        "        return Fg + Fp + Fr, coeffs\n",
        "\n",
        "\n",
        "# === Trainable PFM Model (baseline) ===\n",
        "class PFMOnlyModel(nn.Module):\n",
        "    def __init__(self, goal=(4.2, 4.2), target_avg_speed=4.087,\n",
        "                 speed_tolerance=0.15, num_agents=1000, dt=0.1,\n",
        "                 pred_len=12):\n",
        "        super().__init__()\n",
        "        self.pfm = PotentialField(goal, num_agents)\n",
        "        if target_avg_speed is None:\n",
        "            raise ValueError(\"target_avg_speed required\")\n",
        "        self.target_avg_speed = target_avg_speed\n",
        "        self.speed_tolerance = speed_tolerance\n",
        "        self.min_speed = target_avg_speed * (1 - speed_tolerance)\n",
        "        self.max_speed = target_avg_speed * (1 + speed_tolerance)\n",
        "        self.dt = dt\n",
        "        self.pred_len = pred_len\n",
        "\n",
        "    def apply_speed_constraints(self, preds, last_pos):\n",
        "        B, A, T, _ = preds.shape\n",
        "        out = preds.clone()\n",
        "        cur = last_pos.clone()\n",
        "        for t in range(T):\n",
        "            disp = out[:, :, t] - cur\n",
        "            sp = torch.norm(disp, dim=-1, keepdim=True)\n",
        "            nz = sp > 0\n",
        "            clipped = torch.clamp(sp, self.min_speed, self.max_speed)\n",
        "            sp_final = torch.where(nz, clipped, sp)\n",
        "            dir = disp / (sp + 1e-8)\n",
        "            out[:, :, t] = cur + dir * sp_final\n",
        "            cur = out[:, :, t].clone()\n",
        "        return out\n",
        "\n",
        "    def forward(self, history, neighbors, goal):\n",
        "        B, A, H, _ = history.shape\n",
        "        agent_ids = torch.arange(A).repeat(B, 1).to(history.device)\n",
        "        coeffs = self.pfm.coeff_embedding(agent_ids)\n",
        "\n",
        "        preds = torch.zeros(B, A, self.pred_len, 2, device=history.device)\n",
        "        cur = history[:, :, -1, :].clone()\n",
        "        coeff_list = []\n",
        "\n",
        "        for t in range(self.pred_len):\n",
        "            if t == 0 and H >= 2:\n",
        "                vel = history[:, :, -1, :] - history[:, :, -2, :]\n",
        "                pred_slice = (cur + vel).unsqueeze(2)\n",
        "            elif t == 0:\n",
        "                pred_slice = cur.unsqueeze(2)\n",
        "            else:\n",
        "                pred_slice = preds[:, :, t - 1:t, :].clone()\n",
        "\n",
        "            forces, cstep = self.pfm(cur, pred_slice, neighbors, goal, coeffs)\n",
        "            nextp = cur + forces * self.dt\n",
        "            preds[:, :, t] = nextp\n",
        "            cur = nextp.clone()\n",
        "            coeff_list.append(cstep)\n",
        "\n",
        "        preds = self.apply_speed_constraints(preds, history[:, :, -1, :])\n",
        "        stack = torch.stack(coeff_list, dim=0)\n",
        "        return preds, stack.mean(), stack.var(unbiased=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbiSEdCWDiuA",
        "outputId": "4514ead3-9cd8-4071-8f97-3ef1e5bf900d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎯 Speed Constraints Enabled:\n",
            "   Target Avg Speed: 4.0870\n",
            "   Allowed range: [3.4739, 4.7000]\n",
            "   Tolerance: ±15.0%\n",
            "\n",
            "[0] Loss: 3.6614 | k_att1 μ=1.00, σ²=0.00\n",
            "    Speeds - GT: 0.259, Pred: 3.483, Hist: 0.264\n",
            "    Speed Violations: 1003\n",
            "\n",
            "=== EPOCH 1 SUMMARY ===\n",
            "Avg Loss: 3.6460\n",
            "Avg Speeds: Hist=0.2791, GT=0.2748, Pred=3.4809\n",
            "Speed Error: 3.2062\n",
            "Violations: 34975 | Constraint Compliance: 43.07%\n",
            "========================================\n",
            "[0] Loss: 3.0150 | k_att1 μ=1.00, σ²=0.00\n",
            "    Speeds - GT: 0.275, Pred: 3.481, Hist: 0.279\n",
            "    Speed Violations: 1155\n",
            "\n",
            "=== EPOCH 2 SUMMARY ===\n",
            "Avg Loss: 3.5991\n",
            "Avg Speeds: Hist=0.2775, GT=0.2736, Pred=3.4812\n",
            "Speed Error: 3.2076\n",
            "Violations: 35165 | Constraint Compliance: 42.77%\n",
            "========================================\n",
            "[0] Loss: 3.5499 | k_att1 μ=1.00, σ²=0.00\n",
            "    Speeds - GT: 0.261, Pred: 3.480, Hist: 0.282\n",
            "    Speed Violations: 1056\n",
            "\n",
            "=== EPOCH 3 SUMMARY ===\n",
            "Avg Loss: 3.6214\n",
            "Avg Speeds: Hist=0.2786, GT=0.2736, Pred=3.4812\n",
            "Speed Error: 3.2077\n",
            "Violations: 34916 | Constraint Compliance: 43.17%\n",
            "========================================\n",
            "[0] Loss: 3.4775 | k_att1 μ=1.00, σ²=0.00\n",
            "    Speeds - GT: 0.257, Pred: 3.483, Hist: 0.241\n",
            "    Speed Violations: 1108\n",
            "\n",
            "=== EPOCH 4 SUMMARY ===\n",
            "Avg Loss: 3.6700\n",
            "Avg Speeds: Hist=0.2780, GT=0.2744, Pred=3.4815\n",
            "Speed Error: 3.2071\n",
            "Violations: 35190 | Constraint Compliance: 42.72%\n",
            "========================================\n",
            "[0] Loss: 3.6856 | k_att1 μ=1.00, σ²=0.00\n",
            "    Speeds - GT: 0.306, Pred: 3.482, Hist: 0.302\n",
            "    Speed Violations: 1063\n",
            "\n",
            "=== EPOCH 5 SUMMARY ===\n",
            "Avg Loss: 3.6535\n",
            "Avg Speeds: Hist=0.2770, GT=0.2732, Pred=3.4816\n",
            "Speed Error: 3.2084\n",
            "Violations: 34999 | Constraint Compliance: 43.04%\n",
            "========================================\n",
            "\n",
            "💾 Saving model...\n",
            "✅ Model saved at /content/mta_pfm_trajectory_model_Zara.pth with speed constraints!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_pfm_model(\n",
        "    data_path,\n",
        "    model_save_path,\n",
        "    model_class,\n",
        "    dataset_class,\n",
        "    collate_fn,\n",
        "    batch_size=32,\n",
        "    epochs=3,\n",
        "    learning_rate=0.001,\n",
        "    weight_decay=0.0,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a PFM trajectory prediction model with speed constraints.\n",
        "\n",
        "    Args:\n",
        "        data_path (str): Path to dataset file.\n",
        "        model_save_path (str): Where to save the trained model.\n",
        "        model_class (class): Model class to instantiate (e.g., IntegratedMTAPFMModel).\n",
        "        dataset_class (class): Dataset loader class (e.g., PFM_TrajectoryDataset).\n",
        "        collate_fn (function): Collate function for DataLoader.\n",
        "        batch_size (int): Batch size for DataLoader.\n",
        "        epochs (int): Number of training epochs.\n",
        "        learning_rate (float): Learning rate for optimizer.\n",
        "        weight_decay (float): Weight decay for optimizer.\n",
        "        device (torch.device): Device to train on. If None, auto-select CUDA if available.\n",
        "    \"\"\"\n",
        "\n",
        "    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "    # === DATA LOADING ===\n",
        "    dataset = dataset_class(data_path)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # === MODEL / OPTIMIZER / LOSS ===\n",
        "    model = model_class().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    print(f\"\\n🎯 Speed Constraints Enabled:\")\n",
        "    print(f\"   Target Avg Speed: {model.target_avg_speed:.4f}\")\n",
        "    print(f\"   Allowed range: [{model.min_speed:.4f}, {model.max_speed:.4f}]\")\n",
        "    print(f\"   Tolerance: ±{model.speed_tolerance * 100:.1f}%\\n\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        epoch_gt_speeds, epoch_pred_speeds, epoch_hist_speeds, epoch_violations = [], [], [], []\n",
        "\n",
        "        for batch_idx, (history, future, neighbors, goal) in enumerate(dataloader):\n",
        "            history = history.to(device)\n",
        "            future = future.to(device)\n",
        "            neighbors = neighbors.to(device)\n",
        "            goal = future[:, :, -1, :].clone()  # final target position\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred, coeff_mean, coeff_var = model(history, neighbors, goal)\n",
        "            loss = criterion(pred, future)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # === SPEED TRACKING ===\n",
        "            with torch.no_grad():\n",
        "                gt_speed = calculate_speed(future)\n",
        "                pred_speed = calculate_speed(pred)\n",
        "                hist_speed = calculate_speed(history)\n",
        "\n",
        "                epoch_gt_speeds.append(gt_speed.item())\n",
        "                epoch_pred_speeds.append(pred_speed.item())\n",
        "                epoch_hist_speeds.append(hist_speed.item())\n",
        "\n",
        "                violation_count = check_speed_violations(pred, history, model.min_speed, model.max_speed)\n",
        "                epoch_violations.append(violation_count)\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"[{batch_idx}] Loss: {loss:.4f} | k_att1 μ={coeff_mean:.2f}, σ²={coeff_var:.2f}\")\n",
        "                print(f\"    Speeds - GT: {gt_speed:.3f}, Pred: {pred_speed:.3f}, Hist: {hist_speed:.3f}\")\n",
        "                print(f\"    Speed Violations: {violation_count}\")\n",
        "\n",
        "        # === EPOCH SUMMARY ===\n",
        "        avg_gt_speed = sum(epoch_gt_speeds) / len(epoch_gt_speeds)\n",
        "        avg_pred_speed = sum(epoch_pred_speeds) / len(epoch_pred_speeds)\n",
        "        avg_hist_speed = sum(epoch_hist_speeds) / len(epoch_hist_speeds)\n",
        "        total_violations = sum(epoch_violations)\n",
        "\n",
        "        print(f\"\\n=== EPOCH {epoch+1} SUMMARY ===\")\n",
        "        print(f\"Avg Loss: {epoch_loss/len(dataloader):.4f}\")\n",
        "        print(f\"Avg Speeds: Hist={avg_hist_speed:.4f}, GT={avg_gt_speed:.4f}, Pred={avg_pred_speed:.4f}\")\n",
        "        print(f\"Speed Error: {abs(avg_gt_speed - avg_pred_speed):.4f}\")\n",
        "        print(f\"Violations: {total_violations} | Constraint Compliance: {(1 - total_violations/(len(dataloader)*batch_size*5*12))*100:.2f}%\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "    torch.autograd.set_detect_anomaly(False)\n",
        "\n",
        "    # === SAVE MODEL ===\n",
        "    print(\"\\n💾 Saving model...\")\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epochs,\n",
        "        'loss': epoch_loss/len(dataloader),\n",
        "        'speed_constraints': {\n",
        "            'target_avg_speed': model.target_avg_speed,\n",
        "            'min_speed': model.min_speed,\n",
        "            'max_speed': model.max_speed,\n",
        "            'tolerance': model.speed_tolerance\n",
        "        },\n",
        "        'final_avg_speeds': {\n",
        "            'historical': avg_hist_speed,\n",
        "            'ground_truth': avg_gt_speed,\n",
        "            'predicted': avg_pred_speed\n",
        "        }\n",
        "    }, model_save_path)\n",
        "    print(f\"✅ Model saved at {model_save_path} with speed constraints!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_pfm_model(\n",
        "        data_path=\"/content/crowds_zara02_test_cleaned.txt\",\n",
        "        model_save_path=\"/content/pfm_trajectory_model_Zara.pth\",\n",
        "        model_class=PFMOnlyModel,\n",
        "        dataset_class=PFM_TrajectoryDataset,\n",
        "        collate_fn=collate_fn,\n",
        "        batch_size=32,\n",
        "        epochs=5\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BELJBQOQH2Wm"
      },
      "source": [
        "## train pfm not learnable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G28CMzIjH1Ud"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_pfm_notlearnable_model(\n",
        "    data_path,\n",
        "    model_save_path,\n",
        "    model_class,\n",
        "    dataset_class,\n",
        "    collate_fn,\n",
        "    batch_size=32,\n",
        "    epochs=3,\n",
        "    learning_rate=0.001,\n",
        "    weight_decay=0.0,\n",
        "    device=None,\n",
        "    model_kwargs=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a PFM trajectory prediction model with speed constraints.\n",
        "\n",
        "    Args:\n",
        "        data_path (str): Path to dataset file.\n",
        "        model_save_path (str): Where to save the trained model.\n",
        "        model_class (class): Model class to instantiate (e.g., IntegratedMTAPFMModel).\n",
        "        dataset_class (class): Dataset loader class (e.g., PFM_TrajectoryDataset).\n",
        "        collate_fn (function): Collate function for DataLoader.\n",
        "        batch_size (int): Batch size for DataLoader.\n",
        "        epochs (int): Number of training epochs.\n",
        "        learning_rate (float): Learning rate for optimizer.\n",
        "        weight_decay (float): Weight decay for optimizer.\n",
        "        device (torch.device): Device to train on. If None, auto-select CUDA if available.\n",
        "    \"\"\"\n",
        "\n",
        "    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "    # === DATA LOADING ===\n",
        "    dataset = dataset_class(data_path)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # === MODEL / OPTIMIZER / LOSS ===\n",
        "    model = model_class().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    print(f\"\\n🎯 Speed Constraints Enabled:\")\n",
        "    print(f\"   Target Avg Speed: {model.target_avg_speed:.4f}\")\n",
        "    print(f\"   Allowed range: [{model.min_speed:.4f}, {model.max_speed:.4f}]\")\n",
        "    print(f\"   Tolerance: ±{model.speed_tolerance * 100:.1f}%\\n\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        epoch_gt_speeds, epoch_pred_speeds, epoch_hist_speeds, epoch_violations = [], [], [], []\n",
        "\n",
        "        for batch_idx, (history, future, neighbors, goal) in enumerate(dataloader):\n",
        "            history = history.to(device)\n",
        "            future = future.to(device)\n",
        "            neighbors = neighbors.to(device)\n",
        "            goal = future[:, :, -1, :].clone()  # final target position\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred, coeff_mean, coeff_var = model(history, neighbors, goal)\n",
        "            loss = criterion(pred, future)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # === SPEED TRACKING ===\n",
        "            with torch.no_grad():\n",
        "                gt_speed = calculate_speed(future)\n",
        "                pred_speed = calculate_speed(pred)\n",
        "                hist_speed = calculate_speed(history)\n",
        "\n",
        "                epoch_gt_speeds.append(gt_speed.item())\n",
        "                epoch_pred_speeds.append(pred_speed.item())\n",
        "                epoch_hist_speeds.append(hist_speed.item())\n",
        "\n",
        "                violation_count = check_speed_violations(pred, history, model.min_speed, model.max_speed)\n",
        "                epoch_violations.append(violation_count)\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"[{batch_idx}] Loss: {loss:.4f} | k_att1 μ={coeff_mean:.2f}, σ²={coeff_var:.2f}\")\n",
        "                print(f\"    Speeds - GT: {gt_speed:.3f}, Pred: {pred_speed:.3f}, Hist: {hist_speed:.3f}\")\n",
        "                print(f\"    Speed Violations: {violation_count}\")\n",
        "\n",
        "        # === EPOCH SUMMARY ===\n",
        "        avg_gt_speed = sum(epoch_gt_speeds) / len(epoch_gt_speeds)\n",
        "        avg_pred_speed = sum(epoch_pred_speeds) / len(epoch_pred_speeds)\n",
        "        avg_hist_speed = sum(epoch_hist_speeds) / len(epoch_hist_speeds)\n",
        "        total_violations = sum(epoch_violations)\n",
        "\n",
        "        print(f\"\\n=== EPOCH {epoch+1} SUMMARY ===\")\n",
        "        print(f\"Avg Loss: {epoch_loss/len(dataloader):.4f}\")\n",
        "        print(f\"Avg Speeds: Hist={avg_hist_speed:.4f}, GT={avg_gt_speed:.4f}, Pred={avg_pred_speed:.4f}\")\n",
        "        print(f\"Speed Error: {abs(avg_gt_speed - avg_pred_speed):.4f}\")\n",
        "        print(f\"Violations: {total_violations} | Constraint Compliance: {(1 - total_violations/(len(dataloader)*batch_size*5*12))*100:.2f}%\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "    torch.autograd.set_detect_anomaly(False)\n",
        "\n",
        "    # === SAVE MODEL ===\n",
        "    print(\"\\n💾 Saving model...\")\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epochs,\n",
        "        'loss': epoch_loss/len(dataloader),\n",
        "        'speed_constraints': {\n",
        "            'target_avg_speed': model.target_avg_speed,\n",
        "            'min_speed': model.min_speed,\n",
        "            'max_speed': model.max_speed,\n",
        "            'tolerance': model.speed_tolerance\n",
        "        },\n",
        "        'final_avg_speeds': {\n",
        "            'historical': avg_hist_speed,\n",
        "            'ground_truth': avg_gt_speed,\n",
        "            'predicted': avg_pred_speed\n",
        "        }\n",
        "    }, model_save_path)\n",
        "    print(f\"✅ Model saved at {model_save_path} with speed constraints!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_pfm_model(\n",
        "        data_path=\"/content/crowds_zara02_test_cleaned.txt\",\n",
        "        model_save_path=\"/content/pfm_trajectory_model_Zara.pth\",\n",
        "        model_class=PFMOnlyModel,\n",
        "        dataset_class=PFM_TrajectoryDataset,\n",
        "        collate_fn=collate_fn,\n",
        "        batch_size=32,\n",
        "        epochs=5\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GFN1YVbF8NU"
      },
      "source": [
        "## Test pfm learnable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Am_p3h58GC0r"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_ADE(pred, gt):\n",
        "    # pred: [A, T, 2], gt: [A, T, 2]\n",
        "    return torch.norm(pred - gt, dim=-1).mean().item()\n",
        "\n",
        "def compute_FDE(pred, gt):\n",
        "    # Final step error averaged over agents\n",
        "    return torch.norm(pred[:, -1, :] - gt[:, -1, :], dim=-1).mean().item()\n",
        "\n",
        "def compute_miss_rate(pred, gt, threshold=2.0):\n",
        "    final_dist = torch.norm(pred[:, -1, :] - gt[:, -1, :], dim=-1)\n",
        "    misses = (final_dist > threshold).float()\n",
        "    return misses.mean().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rKokEel6GGS0"
      },
      "outputs": [],
      "source": [
        "def test_with_metrics_pfm(model_path, cleaned_txt_file, model_class):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset = PFM_TrajectoryDataset(cleaned_txt_file)\n",
        "    print(f\"\\n📦 Loaded {len(dataset)} frame samples for evaluation.\")\n",
        "\n",
        "    if len(dataset) == 0:\n",
        "        print(\"❌ No valid samples available. Check your preprocessing.\")\n",
        "        return\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # === LOAD MODEL ===\n",
        "    model = model_class().to(device)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    total_ade, total_fde, total_miss = 0.0, 0.0, 0.0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for history, future, neighbors, goal in dataloader:\n",
        "            history = history.to(device)\n",
        "            future = future.to(device)\n",
        "            neighbors = neighbors.to(device)\n",
        "            goal = goal.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            pred, _, _ = model(history, neighbors, goal)  # [1, A, T, 2]\n",
        "            pred = pred[0]      # [A, T, 2]\n",
        "            future = future[0]  # [A, T, 2]\n",
        "\n",
        "            # Fix length mismatch\n",
        "            min_len = min(pred.size(1), future.size(1))\n",
        "            if pred.size(1) != future.size(1):\n",
        "                print(f\"⚠️ Truncating: pred_len={pred.size(1)}, future_len={future.size(1)} → using {min_len}\")\n",
        "            pred = pred[:, :min_len, :]\n",
        "            future = future[:, :min_len, :]\n",
        "\n",
        "            # === METRICS ===\n",
        "            ade = compute_ADE(pred, future)\n",
        "            fde = compute_FDE(pred, future)\n",
        "            miss = compute_miss_rate(pred, future, threshold=2.0)\n",
        "\n",
        "            total_ade += ade\n",
        "            total_fde += fde\n",
        "            total_miss += miss\n",
        "            count += 1\n",
        "\n",
        "    if count == 0:\n",
        "        print(\"❌ Evaluation aborted: No samples processed.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n📊 Evaluation Metrics on Test Dataset:\")\n",
        "    print(f\"🔹 Average ADE:  {total_ade / count:.4f}\")\n",
        "    print(f\"🔹 Average FDE:  {total_fde / count:.4f}\")\n",
        "    print(f\"🔹 Miss Rate:    {total_miss / count:.4f} (threshold: 2m)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFhNW2cuGUA4",
        "outputId": "8365586f-5f7e-4dfb-8924-f9846555b6d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📦 Loaded 1009 frame samples for evaluation.\n",
            "\n",
            "📊 Evaluation Metrics on Test Dataset:\n",
            "🔹 Average ADE:  4.2697\n",
            "🔹 Average FDE:  5.9976\n",
            "🔹 Miss Rate:    0.6975 (threshold: 2m)\n"
          ]
        }
      ],
      "source": [
        "test_with_metrics_pfm(\n",
        "    model_path=\"/content/pfm_trajectory_model_Zara.pth\",\n",
        "    cleaned_txt_file=\"/content/crowds_zara02_test_cleaned.txt\",\n",
        "    model_class=PFMOnlyModel\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
